{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Thoughts\n",
    "\n",
    "I am excited to work on this project. This is a wonderful opportunity for me to learn about the state-of-the-art technology, Deep Learning, and dream about how I can make impacts on the world.\n",
    "\n",
    "To tackle this project, I have been studying rigorously during my winter break. Well, the \"break\" they said. I finally understood what is Neural Network, Convolution Neural Networks, Back-propagation and few techniques to \"fine-tune\" my network including Adam Gradient Descent, Drop-out, Batch Normalization and Inception. While I understand that this is just a first step to the door, I am gratefully confident about the outcome of this class. \n",
    "\n",
    "Now, let's dive into my steps of how to train the computer regonize Traffic Sign itself. Teehee :d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data\n",
    "\n",
    "I used 'pickle' to load the data. There are training set and test set in dataset. In a perfect world, DO NOT TOUCH test set until you believe that your network is good enough. \n",
    "\n",
    "The way I validated my network is to use cross-validation on minibatches. In particular, for every epoch, I divided my training set into 'BATCH_SIZE' (256) mini-batches. I used 20% of those batches as validation batches. I will go into detail about this in my 'network architechture'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "training_file = '../data/train.p'\n",
    "testing_file = '../data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 2D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Replace each question mark with the appropriate value.\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "image_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "n_classes = len(set(y_train))\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Matplotlib](http://matplotlib.org/) [examples](http://matplotlib.org/examples/index.html) and [gallery](http://matplotlib.org/gallery.html) pages are a great resource for doing visualizations in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Data exploration visualization goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display few Traffic Sign Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.imshow(X_train[1000])\n",
    "plt.figure(2)\n",
    "plt.imshow(X_train[26000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Sign Sample Distribution\n",
    "\n",
    "One might notice the quanity of each traffic sign is not distributed equally. For example, traffic sign ID '1' has much higher amount of images than traffic sign ID '0'. This could be an issue since the network will be more biased toward ID '1'. \n",
    "\n",
    "I will propose a way to solve this issue during data augmemtation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a barchart of frequencies\n",
    "item, count = np.unique(y_train, return_counts=True)\n",
    "freq = np.array((item, count)).T\n",
    "\n",
    "print('%d Observations' % (y_train.shape[0]))\n",
    "plt.bar(item, count, alpha=0.5)\n",
    "plt.title('Unequally Distributed Traffic Sign Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Since the traffic sign samples is not distrubted equally, this could affect the CNN's performance. The network would be biased toward classes with more samples.\n",
    "\n",
    "One possible solution is to do **data augmentation** on the original data. This techquie generates additional data on each traffic sign based on how much data it has. For example, if the average sample for each classes is 1000 images. Any class has less than 300 images need to generate more data. \n",
    "\n",
    "Data Augmentation includes but not limited to *blurring, rotating, shifting, darking* on original images.\n",
    "\n",
    "For image transformation, I found this documentation is helpful: http://docs.opencv.org/trunk/da/d6e/tutorial_py_geometric_transformations.html\n",
    "\n",
    "This library seems promising. However, I want to implement the code once before using it:\n",
    "https://github.com/aleju/imgaug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Transformation Methods\n",
    "\n",
    "Before generating data, we need to write some methods to transform orginal image. Below are my proposed methods: translation, rotation, affirm transformation, blur, brighten, darken an image.\n",
    "\n",
    "All the methods will generate new image at different random levels (using numpy.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def translate(img):\n",
    "    x = img.shape[0]\n",
    "    y = img.shape[1]\n",
    "\n",
    "    x_shift = np.random.uniform(-0.3 * x, 0.3 * x)\n",
    "    y_shift = np.random.uniform(-0.3 * y, 0.3 * y)\n",
    "\n",
    "    shift_matrix = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
    "    shift_img = cv2.warpAffine(img, shift_matrix, (x, y))\n",
    "\n",
    "    return shift_img\n",
    "\n",
    "\n",
    "def rotate(img):\n",
    "    row, col, channel = img.shape\n",
    "\n",
    "    angle = np.random.uniform(-180, 180)\n",
    "    rotation_point = (row / 2, col / 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(rotation_point, angle, 1)\n",
    "\n",
    "    rotated_img = cv2.warpAffine(img, rotation_matrix, (col, row))\n",
    "    return rotated_img\n",
    "\n",
    "\n",
    "def shear(img):\n",
    "    x, y, channel = img.shape\n",
    "\n",
    "    shear = np.random.randint(5,15)\n",
    "    pts1 = np.array([[5, 5], [20, 5], [5, 20]]).astype('float32')\n",
    "    pt1 = 5 + shear * np.random.uniform() - shear / 2\n",
    "    pt2 = 20 + shear * np.random.uniform() - shear / 2\n",
    "    pts2 = np.float32([[pt1, 5], [pt2, pt1], [5, pt2]])\n",
    "\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    result = cv2.warpAffine(img, M, (y, x))\n",
    "    return result\n",
    "\n",
    "\n",
    "def blur(img):\n",
    "    r_int = np.random.randint(0, 2)\n",
    "    odd_size = 2 * r_int + 1\n",
    "    return cv2.GaussianBlur(img, (odd_size, odd_size), 0)\n",
    "\n",
    "\n",
    "def gamma(img):\n",
    "    gamma = np.random.uniform(0.3, 1.5)\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    new_img = cv2.LUT(img, table)\n",
    "    return new_img\n",
    "\n",
    "def pre_process(imgs):\n",
    "    # imgs = imgs - np.mean(imgs)\n",
    "    imgs  /= np.std(imgs, axis=0)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test on a sample.\n",
    "\n",
    "I generate 5 images per transformation on a random image of training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 22700\n",
    "\n",
    "sample_1 = X_train[i]\n",
    "plt.figure(1)\n",
    "plt.imshow(sample_1), plt.title('Input')\n",
    "test = pre_process(X_train)\n",
    "plt.figure(2)\n",
    "plt.imshow(test[22700],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 22700\n",
    "sample_1 = X_train[i]\n",
    "plt.figure(1)\n",
    "plt.imshow(sample_1), plt.title('Input')\n",
    "\n",
    "translated_img = []\n",
    "rotated_img = []\n",
    "shear_img = []\n",
    "blur_img = []\n",
    "noise_img = []\n",
    "gamma_img=[]\n",
    "for i in range(5):\n",
    "    translated_img.append(translate(sample_1))\n",
    "    rotated_img.append(rotate(sample_1))\n",
    "    shear_img.append(shear(sample_1))\n",
    "    blur_img.append(blur(sample_1))\n",
    "    gamma_img.append(gamma(sample_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize these images using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = 5\n",
    "col = 5\n",
    "plt.subplot(row, col,1), plt.title('Shift')\n",
    "plt.subplot(row, col,2), plt.title('Rotation')\n",
    "plt.subplot(row, col,3), plt.title('Shear')\n",
    "plt.subplot(row, col,4), plt.title('Blur')\n",
    "plt.subplot(row, col,5), plt.title('Gamma')\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    pos = 5*i\n",
    "    plt.subplot(row, col,pos+1), plt.imshow(translated_img[i]),  plt.axis('off')\n",
    "    plt.subplot(row, col,pos+2), plt.imshow(rotated_img[i]), plt.axis('off') \n",
    "    plt.subplot(row, col,pos+3), plt.imshow(shear_img[i]), plt.axis('off')\n",
    "    plt.subplot(row, col,pos+4), plt.imshow(blur_img[i]), plt.axis('off')  \n",
    "    plt.subplot(row, col,pos+5), plt.imshow(gamma_img[i]), plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, now we create an algorithm to randomly select which transformation will be applied on image. That means one image could have multiple transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_transform(img):\n",
    "    # There are total of 6 transformation\n",
    "    # I will create an boolean array of 6 elements [ 0 or 1]\n",
    "    a = np.random.randint(0, 2, [1, 5]).astype('bool')[0]\n",
    "    if a[0] == 1: img = translate(img)\n",
    "    if a[1] == 1: img = rotate(img)\n",
    "    if a[2] == 1: img = shear(img)\n",
    "    if a[3] == 1: img = blur(img)\n",
    "    if a[4] == 1: img = gamma(img)\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on 30 km sign again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "    pos=5*j\n",
    "    for i in range(8):\n",
    "        test = random_transform(sample_1)\n",
    "        plt.subplot(5, 7, pos+i+1), plt.imshow(test), plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate New Dataset\n",
    "\n",
    "We have a way to generate additional data now. However, each class in dataset should have different needed amount of additional data.\n",
    "\n",
    "In this step, I will:\n",
    "- Separate each class dataset.\n",
    "- Compare number of dataset to average dataset/class.\n",
    "- Based on that ration, I will decide to add more/less data into the current class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Separate Traffic Sign Data Set:\n",
    "For every traffic sign, I decided to place in different arrays so that I could expand these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_traffic_signs = len(set(y_train))\n",
    "print('There is total of ', total_traffic_signs, ' traffic signs.')\n",
    "\n",
    "# Calculate how many images in one traffic sign\n",
    "ts, imgs_per_sign = np.unique(y_train, return_counts=True)\n",
    "avg_img_per_sign = np.ceil(np.mean(imgs_per_sign)).astype('uint32')\n",
    "print(avg_img_per_sign, \" is an average number of images per traffic sign.\")\n",
    "\n",
    "\n",
    "# Based on that average, we can estimate how many images a traffic sign need to have\n",
    "# First, separate each traffic sign training set into different arrays\n",
    "separated_data = []\n",
    "for traffic_sign in range(total_traffic_signs):\n",
    "    images_in_this_sign = X_train[y_train == traffic_sign,...]\n",
    "    separated_data.append(images_in_this_sign)\n",
    "\n",
    "print(\"\\nIn the first traffic sign.\")\n",
    "print(\"There is \", separated_data[0].shape[0], ' images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate New Data Set:\n",
    "So we have just separated each traffic sign into mini data sets instead of a giant 'X_train'.\n",
    "\n",
    "For each mini data set, I used a so-called 'scale-factor' to demtermine how more data should I add to the original dataset. 'Scale-factor' is calculated by (Current Images/Average Images per set)*2 . \n",
    "\n",
    "\n",
    "The more data, the merrier. \n",
    "\n",
    "**This step takes some time. Be patient :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Second, for each dataset, I generate new images randomly based on current total images       \n",
    "expanded_data = np.array(np.zeros((1, 32,32,3)))\n",
    "expanded_labels = np.array([0])\n",
    "\n",
    "for sign, sign_images in enumerate(separated_data):\n",
    "    scale_factor = (2*(avg_img_per_sign/imgs_per_sign[sign])).astype('uint32')\n",
    "    new_images = []\n",
    "\n",
    "    # Generate new images  <---- Could apply list comprehension here\n",
    "    for img in sign_images:\n",
    "        for _ in range(scale_factor):\n",
    "            new_images.append(random_transform(img))\n",
    "\n",
    "    # Add old images and new images into 1 array    \n",
    "    if len(new_images) > 0:\n",
    "        sign_images = np.concatenate((sign_images, new_images),axis=0)\n",
    "    new_labels = np.full(len(sign_images),sign, dtype ='uint8')\n",
    "    # Insert new_images to current dataset\n",
    "    expanded_data = np.concatenate((expanded_data,sign_images), axis = 0)\n",
    "    expanded_labels = np.concatenate((expanded_labels, new_labels), axis=0)   \n",
    "\n",
    "print(expanded_data.shape)\n",
    "print(expanded_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Before and After Data Augmentation:\n",
    "\n",
    "We just finished data augmentation. Let's visualize how our data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a barchart of frequencies\n",
    "item, count = np.unique(y_train, return_counts=True)\n",
    "freq = np.array((item, count)).T\n",
    "\n",
    "print('Before Data Augmentation: %d Observations' % (y_train.shape[0]))\n",
    "plt.figure(1)\n",
    "plt.bar(item, count, alpha=0.5)\n",
    "plt.title('Before Data Augmentation: Unequally Distributed Data')\n",
    "\n",
    "item2, count2 = np.unique(expanded_labels, return_counts=True)\n",
    "freq2 = np.array((item2, count2)).T\n",
    "\n",
    "print('After Data Augmentation: %d Observations' % (expanded_labels.shape[0]))\n",
    "plt.figure(2)\n",
    "plt.bar(item2, count2, alpha=0.5)\n",
    "plt.title('After Data Augmentation: More Equally Distributed Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pre-Processing Data\n",
    "\n",
    "Before loading data into the CNN, I need to pre-process the dataset. One of important pre-process is 'zero-centered' data. Though many applied 'grayscale' images for training, I beleive CNN should be able to recognize colorful images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = expanded_data, expanded_labels\n",
    "\n",
    "X_train = X_train - np.mean(X_train)\n",
    "X_train = X_train/np.std(X_train,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "My proposed pipeline is:\n",
    "\n",
    "- Pre-process data : Perform data augmentation, normalization on Y channel of YUV color space (Yann Lecun)\n",
    "- Network Architecture: VGG-16\n",
    "- \n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these.\n",
    "\n",
    "**NOTE:** The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Preprocess the data here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe how you preprocessed the data. Why did you choose that technique?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Generate data additional data (OPTIONAL!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. **Optional**: If you generated additional data, how did you generate the data? Why did you generate the data? What are the differences in the new dataset (with generated data) from the original dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem? It may have been a process of trial and error, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think this is suitable for the current problem._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It could be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures when compared to testing on the dataset? The simplest way to do this check the accuracy of the predictions. For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate._\n",
    "\n",
    "_**NOTE:** You could check the accuracy manually by using `signnames.csv` (same directory). This file has a mapping from the class id (0-42) to the corresponding sign name. So, you could take the class id the model outputs, lookup the name in `signnames.csv` and see if it matches the sign from the image._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n",
    "\n",
    "`tf.nn.top_k` will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "\n",
    "Take this numpy array as an example:\n",
    "\n",
    "```\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "```\n",
    "\n",
    "Running it through `sess.run(tf.nn.top_k(tf.constant(a), k=3))` produces:\n",
    "\n",
    "```\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "```\n",
    "\n",
    "Looking just at the first row we get `[ 0.34763842,  0.24879643,  0.12789202]`, you can confirm these are the 3 largest probabilities in `a`. You'll also notice `[3, 0, 5]` are the corresponding indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
